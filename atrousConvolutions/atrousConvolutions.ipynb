{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Atrous convolutions for fun and profit. OK maybe just for fun.\n",
    "\n",
    "\n",
    "# numpy implementation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load input data (we will train the network as a shallow autoencoder for starters)\n",
    "\n",
    "\n",
    "# parameters\n",
    "hSize = 64\n",
    "kernSize = 3 # kernels are 3x3 \n",
    "\n",
    "\n",
    "# initialize the weights\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# back prop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/main/anaconda3/envs/lV/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow implementation\n",
    "\n",
    "# imports \n",
    "import numpy as np\n",
    "# Used for reading in images\n",
    "import cv2\n",
    "import scipy.misc as misc\n",
    "# used for timing \n",
    "import time\n",
    "\n",
    "# tensorflow imports for flowing those tensors\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "\n",
    "# plotting imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# hyperparameters\n",
    "mySeed = 1337\n",
    "\n",
    "dispIt = 40\n",
    "maxSteps = 200\n",
    "dORate = 0.4\n",
    "batchSize = 4\n",
    "lR = 3e-4\n",
    "# number of kernels per layer\n",
    "convDepth = 4\n",
    "myBias = 0#1.0\n",
    "#dimX = 482\n",
    "#dimY = 646\n",
    "# Image characteristics\n",
    "dimY = 1344#imgWidth\n",
    "dimX = 1024#imgHeight\n",
    "myChan = 1\n",
    "myOutChan = 1\n",
    "# ***\n",
    "pool1Size = 1#2   \n",
    "pool2Size = 1\n",
    "kern1Size = 3\n",
    "kern2Size = 3\n",
    "\n",
    "\n",
    "data = tf.placeholder(\"float\",[None, dimX,dimY,myChan], name='X')\n",
    "\n",
    "learningRate = tf.placeholder(\"float\",name='learningRate')\n",
    "\n",
    "mode = tf.placeholder(\"bool\",name=\"myMode\")\n",
    "\n",
    "# Define atrous conv filtes\n",
    "a1Filters = tf.Variable(tf.random_normal([3, 3,convDepth,convDepth], stddev=0.35),name=\"a1weights\")\n",
    "a2Filters = tf.Variable(tf.random_normal([3, 3,convDepth,convDepth], stddev=0.35),name=\"a2weights\")\n",
    "a4Filters = tf.Variable(tf.random_normal([3, 3,convDepth,convDepth], stddev=0.35),name=\"a4weights\")\n",
    "a8Filters = tf.Variable(tf.random_normal([3, 3,convDepth,convDepth], stddev=0.35),name=\"a8weights\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def atrousCNN(data,mode):\n",
    "\t# mode = false apply dropout\n",
    "\t# mode = true don't apply dropout, i.e. for evaluation/test\n",
    "\t\n",
    "\tinputLayer = tf.reshape(data,[-1,dimX,dimY,myChan])\n",
    "\t\n",
    "\t\"\"\"Layer 0\"\"\"\n",
    "\tconv0 = tf.layers.conv2d(\n",
    "\t\tinputs = inputLayer,\n",
    "\t\tfilters = convDepth,\n",
    "\t\tkernel_size = [kern1Size,kern1Size],\n",
    "\t\tpadding = \"same\",\n",
    "\t\tactivation = tf.nn.relu, use_bias = True, bias_initializer = tf.constant_initializer(myBias),name = \"conv0\")\n",
    "\tdropout0 = tf.layers.dropout(\n",
    "\t    inputs = conv0,#conv1,\n",
    "\t    rate = dORate,\n",
    "\t    training = mode,\n",
    "\t    name = \"dropout0\")#128x16\n",
    "\n",
    "\t\"\"\"Layer 1\"\"\"\n",
    "\tconv1 = tf.layers.conv2d(\n",
    "\t\tinputs = dropout0,\n",
    "\t\tfilters = convDepth,\n",
    "\t\tkernel_size = [kern1Size,kern1Size],\n",
    "\t\tpadding = \"same\",\n",
    "\t\tactivation = tf.nn.relu, use_bias = True, bias_initializer = tf.constant_initializer(myBias),name = \"conv1\")\n",
    "\tdropout1 = tf.layers.dropout(\n",
    "\t    inputs = conv1,#conv1,\n",
    "\t    rate = dORate,\n",
    "\t    training = mode,\n",
    "\t    name = \"dropout1\")#128x16\n",
    "\n",
    "\t\"\"\"Layer 2\"\"\"\n",
    "\tconv2 = tf.layers.conv2d(\n",
    "\t\tinputs = dropout1,\n",
    "\t\tfilters = convDepth,\n",
    "\t\tkernel_size = [kern1Size,kern1Size],\n",
    "\t\tpadding = \"same\",\n",
    "\t\tactivation = tf.nn.relu, use_bias = True, bias_initializer = tf.constant_initializer(myBias),name = \"conv2\")\n",
    "\tdropout2 = tf.layers.dropout(\n",
    "\t    inputs = conv2,#conv1,\n",
    "\t    rate = dORate,\n",
    "\t    training = mode,\n",
    "\t    name = \"dropout2\")#128x16\n",
    "\n",
    "\t\"\"\"Parallel atrous convolutions\"\"\"\n",
    "\tatrous1 = tf.nn.relu(tf.nn.atrous_conv2d(dropout2,a1Filters,1,\"SAME\",name='atrous1'))\n",
    "\tatrous2 = tf.nn.relu(tf.nn.atrous_conv2d(dropout2,a2Filters,2,\"SAME\",name='atrous2'))\n",
    "\tatrous4 = tf.nn.relu(tf.nn.atrous_conv2d(dropout2,a4Filters,4,\"SAME\",name='atrous4'))\n",
    "\tatrous8 = tf.nn.relu(tf.nn.atrous_conv2d(dropout2,a8Filters,8,\"SAME\",name='atrous8'))\n",
    "\t\n",
    "\tdropout3_5 = tf.layers.dropout(\n",
    "\t\tinputs = tf.concat([atrous1,atrous2,atrous4,atrous8],3),\n",
    "#([atrous1,atrous2,atrous3,atrous4,atrous5,atrous6,atrous7,atrous8,atrous9,atrous10],3),\n",
    "\t\trate = dORate,\n",
    "\t\ttraining = mode,\n",
    "\t\tname = \"dropout3_5\")\n",
    "\tconv4 = tf.layers.conv2d(\n",
    "\t\tinputs = dropout3_5,\n",
    "\t\tfilters = convDepth,\n",
    "\t\tkernel_size = [kern1Size,kern1Size],\n",
    "\t\tpadding = \"same\",\n",
    "\t\tactivation = tf.nn.relu, \n",
    "\t\tuse_bias = True, \n",
    "\t\tbias_initializer = tf.constant_initializer(myBias),name = \"conv4\")\n",
    "    \n",
    "\tdropout4 = tf.layers.dropout(\n",
    "\t\tinputs = tf.concat([atrous1,atrous2,atrous4,atrous8],3),\n",
    "#([atrous1,atrous2,atrous3,atrous4,atrous5,atrous6,atrous7,atrous8,atrous9,atrous10],3),\n",
    "\t\trate = dORate,\n",
    "\t\ttraining = mode,\n",
    "\t\tname = \"dropout4\")\n",
    "\t\n",
    "\tconv5 = tf.layers.conv2d(\n",
    "\t\tinputs = dropout4,\n",
    "\t\tfilters = 1,#convDepth,\n",
    "\t\tkernel_size = [kern1Size,kern1Size],\n",
    "\t\tpadding = \"same\",\n",
    "\t\tactivation = tf.nn.relu, \n",
    "\t\tuse_bias = True, \n",
    "\t\tbias_initializer = tf.constant_initializer(myBias),name = \"conv5\")\n",
    "\tmyOutput = conv5\n",
    "\treturn myOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape (?, 1024, 1344, 1)\n",
      "WARNING:tensorflow:From <ipython-input-5-574fb8da1696>:14: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n"
     ]
    }
   ],
   "source": [
    "myOut = atrousCNN(data,mode)\n",
    "\n",
    "print(\"output shape\",np.shape(myOut))\t\n",
    "\n",
    "# operating in autoencoder mode\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.pow(data - myOut, 2)))\n",
    "\n",
    "#loss = tf.reduce_mean(targets - myOut)\n",
    "trainOp = tf.train.AdamOptimizer(\n",
    "\tlearning_rate=lR,beta1=0.9,\n",
    "\tbeta2 = 0.999,\n",
    "\tepsilon=1e-08,\n",
    "\tuse_locking=False,\n",
    "\tname='Adam').minimize(loss,global_step = tf.contrib.framework.get_global_step())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init = tf.global_variables_initializer()\n",
    "def main(unused_argv):\n",
    "    #tf.reset_default_graph()\n",
    "    with tf.Session() as sess: \n",
    "        #tf.initialize_all_variables().run() \n",
    "        sess.run(init)\n",
    "        lR = 3e-5\n",
    "        myX = np.load('./coelhoData.npy')\n",
    "        myMin = np.min(myX)\n",
    "\n",
    "        myMax = np.max(myX-myMin)\n",
    "        myX = (myX-myMin)/(myMax)\n",
    "        myX = np.reshape(myX, (myX.shape[0],myX.shape[1],myX.shape[2],1))\n",
    "\n",
    "        for i in range(maxSteps):\n",
    "            for ck in range(0,len(myX),batchSize):\n",
    "                input_ = myX[ck:ck+batchSize]\n",
    "\n",
    "                sess.run(trainOp, feed_dict = {data: input_, learningRate: lR, mode: True})\n",
    "\n",
    "            if(i% dispIt ==0):\n",
    "                inp = tf.placeholder(tf.float32)\n",
    "\n",
    "                myMean = tf.reduce_mean(inp)\n",
    "                myTemp = (sess.run(loss, feed_dict={data: input_, learningRate: lR, mode: False}))\n",
    "                myLossTrain = myMean.eval(feed_dict={inp: myTemp})\n",
    "                print(\"Epoch %i training loss: %.4e \"%(i,myLossTrain))\n",
    "\n",
    "                recon = sess.run(myOut,feed_dict = {data: input_, mode: False})\n",
    "                #decon = sess.run(myDecon,feed_dict = {data: input_, mode: False})\n",
    "                plt.figure(figsize=(10,10))\n",
    "                for ck in range(3):\n",
    "                    \n",
    "                    plt.subplot(3,2,ck*2+1)\n",
    "                    plt.title(\"original image\")\n",
    "                    plt.imshow(input_[ck,:,:,0],cmap=\"inferno\")\n",
    "                    plt.subplot(3,2,ck*2+2)\n",
    "                    plt.title(\"reconstructed\")\n",
    "                    plt.imshow(recon[ck,:,:,0],cmap=\"inferno\")\n",
    "\n",
    "                    #plt.subplot(133)\n",
    "                    #plt.title(\"pseudo-deconvolved\")\n",
    "                    #plt.imshow(decon[ck,:,:,0],cmap=\"inferno\")\n",
    "\n",
    "                plt.show()#plt.savefig(\"./figs/epoch%iImg%i.png\"%(i,ck))\n",
    "                plt.clf()\n",
    "\n",
    "    print(\"finished .. . .\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lV: Visual Learning",
   "language": "python",
   "name": "lv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
